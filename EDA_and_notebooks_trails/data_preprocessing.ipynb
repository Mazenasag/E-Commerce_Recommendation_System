{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befad4ea",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">2.  üßπ Data Preprocessing\n",
    "\n",
    "This notebook performs comprehensive data preprocessing on the e-commerce recommendation dataset.\n",
    "\n",
    "## Objectives and summary:\n",
    "1. Load the raw dataset and standardize column names\n",
    "2. Clean data fields: trim strings, lowercase text, and format dates\n",
    "3. Handle missing Event_Date:\n",
    "   - Identify rows with missing dates\n",
    "   - Summarize total missing rows, unique users, and products\n",
    "   - Optionally summarize by event type\n",
    "   - Fill missing dates with median date or forward-fill to preserve patterns\n",
    "4. Remove invalid or redundant columns (e.g., index)\n",
    "5. Segment users into warm (2+ interactions) and cold (1 interaction)\n",
    "6. Clean product names:\n",
    "   - Remove punctuation, numbers, and stop words\n",
    "   - Normalize units (e.g., ML ‚Üí ŸÖŸÑ, KG ‚Üí ŸÉŸäŸÑŸà)\n",
    "   - Tokenize and generate cleaned word lists\n",
    "7. Count units in product names and generate unit frequency summary\n",
    "8. Detect product name language (Arabic, English, Mixed)\n",
    "9. Apply normalized and cleaned text in-place for further analysis\n",
    "10. Generate summaries for user segmentation, product vocabulary, and language distribution\n",
    "11. Encode IDs for modeling:\n",
    "    - Convert `customer_id` ‚Üí `user_idx` and `product_id` ‚Üí `product_idx`\n",
    "    - Create mapping dictionaries for IDs and indices\n",
    "    - Store the number of unique users and products\n",
    "    - Essential for matrix-based or embedding-based recommendation models\n",
    "12. Robust user segmentation:\n",
    "    - Identify warm and cold users based on configurable threshold (`warm_user_threshold`)\n",
    "    - Returns actual sets of warm and cold users for downstream analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fe7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For fast similarity search\n",
    "import faiss\n",
    "\n",
    "# For models\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úÖ All libraries loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23583a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìÇ DATA LOADED (NO FILTERING)\n",
      "======================================================================\n",
      "   Total Interactions: 500,000\n",
      "   Total Users: 433,787\n",
      "   Total Products: 200,325\n",
      "\n",
      "üìä User Segmentation:\n",
      "   Warm users (2+ interactions): 49,359 (11.4%)\n",
      "   Cold users (1 interaction): 384,428 (88.6%)\n",
      "    433,787 users!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD DATA (NO FILTERING - KEEP ALL USERS!)\n",
    "# ============================================================\n",
    "DATA_PATH = \"..\\\\data\\\\raw\\\\csv_for_case_study_V1.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Clean\n",
    "df['event'] = df['event'].str.lower().str.strip()\n",
    "if 'index' in df.columns:\n",
    "    df = df.drop('index', axis=1)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìÇ DATA LOADED (NO FILTERING)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Total Interactions: {len(df):,}\")\n",
    "print(f\"   Total Users: {df['customer_id'].nunique():,}\")\n",
    "print(f\"   Total Products: {df['product_id'].nunique():,}\")\n",
    "\n",
    "# Identify warm vs cold users\n",
    "user_counts = df.groupby('customer_id').size()\n",
    "warm_users = set(user_counts[user_counts >= 2].index)\n",
    "cold_users = set(user_counts[user_counts == 1].index)\n",
    "\n",
    "print(f\"\\nüìä User Segmentation:\")\n",
    "print(f\"   Warm users (2+ interactions): {len(warm_users):,} ({len(warm_users)/len(user_counts)*100:.1f}%)\")\n",
    "print(f\"   Cold users (1 interaction): {len(cold_users):,} ({len(cold_users)/len(user_counts)*100:.1f}%)\")\n",
    "print(f\"    {len(user_counts):,} users!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6080716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_id', 'customer_id', 'product_name', 'event_date', 'event'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cda3f9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id          0\n",
       "customer_id         0\n",
       "product_name        0\n",
       "event_date      22179\n",
       "event               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c5f4a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with missing Event_Date: 22179\n",
      "Unique users: 20901\n",
      "Unique products: 17011\n",
      "Total interactions (events) missing date: 22179\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interactions</th>\n",
       "      <th>unique_users</th>\n",
       "      <th>unique_products</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wishlist</th>\n",
       "      <td>22179</td>\n",
       "      <td>20901</td>\n",
       "      <td>17011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          interactions  unique_users  unique_products\n",
       "event                                                \n",
       "wishlist         22179         20901            17011"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows with missing Event_Date\n",
    "missing_event_df = df[df['event_date'].isnull()]\n",
    "\n",
    "# Basic summary\n",
    "total_rows_missing = len(missing_event_df)\n",
    "unique_users_missing = missing_event_df['customer_id'].nunique()\n",
    "unique_products_missing = missing_event_df['product_id'].nunique()\n",
    "total_interactions_missing = missing_event_df.shape[0]\n",
    "\n",
    "print(f\"Total rows with missing Event_Date: {total_rows_missing}\")\n",
    "print(f\"Unique users: {unique_users_missing}\")\n",
    "print(f\"Unique products: {unique_products_missing}\")\n",
    "print(f\"Total interactions (events) missing date: {total_interactions_missing}\")\n",
    "\n",
    "# Optional: summary by event type\n",
    "summary_by_event = (\n",
    "    missing_event_df.groupby('event')\n",
    "    .agg(\n",
    "        interactions=('event', 'count'),\n",
    "        unique_users=('customer_id', 'nunique'),\n",
    "        unique_products=('product_id', 'nunique')\n",
    "    )\n",
    "    .sort_values('interactions', ascending=False)\n",
    ")\n",
    "\n",
    "summary_by_event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d588243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac24a1",
   "metadata": {},
   "source": [
    "This step addresses missing values in the dataset:\n",
    "\n",
    "- Most of the data is already near the **median**  \n",
    "- Using **forward-fill (`ffill()`)** preserves the **original pattern** of interactions  \n",
    "- Maintains the **normal distribution** of the data  \n",
    "- Avoids distorting sequential trends or user/product behavior  \n",
    "- Ensures data integrity for downstream analysis and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06e430b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f226c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_id', 'customer_id', 'product_name', 'event_date', 'event'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1d3f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = \" \".join(df['product_name'].astype(str)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f6ca9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÿ≥ÿßÿ¶ŸÑ',\n",
       " 'ÿ∫ÿ≥ŸäŸÑ',\n",
       " 'ŸÑŸÑŸÖŸÑÿßÿ®ÿ≥',\n",
       " 'ÿ∑ÿ®ŸäÿπŸä',\n",
       " 'ÿπÿØÿØ',\n",
       " '2',\n",
       " 'ÿπÿ®Ÿàÿ©',\n",
       " 'ÿ®ÿ≠ÿ¨ŸÖ',\n",
       " '1.8',\n",
       " 'ŸÑÿ™ÿ±',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '002',\n",
       " 'ÿ±Ÿäÿ¨ŸÑŸäÿ≤',\n",
       " 'ÿ®Ÿàÿ®ÿß',\n",
       " 'ÿ®Ÿàÿ®ÿß',\n",
       " 'ÿ¥ÿ±Ÿäÿ∑',\n",
       " 'ŸÑÿ®ÿßŸÜ',\n",
       " 'ÿπŸÑŸÉÿ©',\n",
       " 'ÿ®ŸÜŸÉŸáÿ©',\n",
       " 'ÿßŸÑŸÅÿ±ÿßŸàŸÑÿ©',\n",
       " '56',\n",
       " 'ÿ¨ŸÖ',\n",
       " 'ÿ≥ÿßÿπÿ©',\n",
       " 'ŸäÿØ',\n",
       " 'ÿ±ÿ¨ÿßŸÑŸäÿ©',\n",
       " 'M34-2',\n",
       " 'ŸÅŸàÿ∑',\n",
       " 'ÿ™ŸÜÿ∏ŸäŸÅ',\n",
       " 'ŸÖŸäŸÉÿ±ŸàŸÅÿßŸäÿ®ÿ±',\n",
       " 'ÿ±ŸäŸÉÿ≥Ÿà',\n",
       " '-',\n",
       " '6',\n",
       " 'ÿ≠ÿ®ÿßÿ™',\n",
       " 'ÿπÿ®ÿßŸäÿ©',\n",
       " 'ŸÉŸÑÿßÿ≥ŸäŸÉ',\n",
       " 'ŸÖÿ∑ÿ±ÿ≤Ÿá',\n",
       " 'ÿ®ÿßŸÑÿ≥ÿØŸà',\n",
       " 'ÿßŸÑÿßÿ≥ŸàÿØ',\n",
       " '(ÿ≥ÿØŸà',\n",
       " '47)',\n",
       " '-',\n",
       " 'ÿ∑ŸÑÿ®',\n",
       " 'ŸÖÿ≥ÿ®ŸÇ',\n",
       " 'ÿ¨Ÿáÿßÿ≤',\n",
       " 'ÿßŸÑÿπÿßÿ®',\n",
       " 'ŸÅŸäÿØŸäŸà',\n",
       " 'ÿ±ŸÇŸÖŸä',\n",
       " 'ŸÖÿπ',\n",
       " 'ÿßÿ≤ÿ±ÿßÿ±',\n",
       " 'ÿßŸÑÿ™ÿ≠ŸÉŸÖ',\n",
       " 'ÿßŸÑŸÑÿßÿ≥ŸÑŸÉŸäÿ©',\n",
       " '(64G)',\n",
       " 'ÿ™ÿ¥ŸÇŸäÿ±',\n",
       " 'ÿ≠Ÿàÿßÿ¨ÿ®',\n",
       " 'ÿ®ÿØŸàŸÜ',\n",
       " 'ÿ±ÿ™Ÿàÿ¥',\n",
       " 'ÿßŸÑÿµÿßÿ®ŸàŸÜ',\n",
       " 'ÿßŸÑŸÖÿ∫ÿ±ÿ®Ÿä',\n",
       " 'ÿßŸÑÿ®ŸÑÿØŸä',\n",
       " 'ÿ®ÿ≤Ÿäÿ™',\n",
       " 'ÿßŸÑÿßÿ±ÿ∫ÿßŸÜ',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿ¨ÿßÿ±ÿØŸÜ',\n",
       " 'ÿßŸàŸÑŸäÿßŸÜ',\n",
       " '-',\n",
       " '500',\n",
       " 'ÿ¨ŸÖ',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '010',\n",
       " 'ŸÖÿ≥ŸÉÿ±Ÿá',\n",
       " 'ÿ≠Ÿàÿßÿ¨ÿ®',\n",
       " 'ÿ¥ŸÅÿßŸÅŸá',\n",
       " 'ŸÜÿßŸä',\n",
       " 'ÿØÿ±ŸäŸÖ',\n",
       " 'ÿßŸÇÿ±ÿßÿµ',\n",
       " 'ŸÇÿ∑ŸÜŸäÿ©',\n",
       " 'ŸÜŸÇŸäÿ©',\n",
       " 'ŸÑŸÑÿ™ÿ¨ŸÖŸäŸÑ',\n",
       " 'ÿ¨ŸàŸÜÿ≥ŸàŸÜ',\n",
       " 'ŸàŸÉÿ≥',\n",
       " 'ŸÖÿ≥ÿßÿ¨',\n",
       " 'ÿ™ÿØŸÑŸäŸÉ',\n",
       " 'ÿßŸÑÿπÿ∂ŸÑÿßÿ™',\n",
       " 'ÿßŸÑŸÑÿßÿ≥ŸÑŸÉŸä',\n",
       " 'ŸÖÿπ',\n",
       " '6',\n",
       " 'ÿ≥ÿ±ÿπÿßÿ™',\n",
       " 'Ÿà',\n",
       " '8',\n",
       " 'ÿ±ÿ§Ÿàÿ≥ÿå',\n",
       " 'ÿßÿÆÿ∂ÿ±',\n",
       " 'ŸÖÿπ',\n",
       " 'ÿßÿ≥ŸàÿØ',\n",
       " 'ŸÑÿ®ÿßÿØ',\n",
       " 'ÿßŸÑÿ∫ŸäŸÖÿ©',\n",
       " '200√ó200',\n",
       " 'ÿßÿ±ÿ™ŸÅÿßÿπ',\n",
       " '12',\n",
       " 'ÿ≥ŸÖ',\n",
       " 'ÿπÿ®ÿßŸäÿ©',\n",
       " 'ŸÉŸÑŸàÿ¥',\n",
       " 'ÿ®ŸÇŸÖÿßÿ¥',\n",
       " 'ŸÜÿßÿπŸÖ',\n",
       " 'ÿπŸÑŸâ',\n",
       " 'ÿÆÿ∑Ÿàÿ∑',\n",
       " 'ÿ∑ŸàŸÑŸäÿ©',\n",
       " 'ŸÉÿ®ÿ≥ŸàŸÑÿßÿ™',\n",
       " 'ÿ≥ÿ™ÿßÿ±ÿ®ŸÉÿ≥',\n",
       " 'ÿßŸÖÿ±ŸäŸÉÿßŸÜŸà',\n",
       " 'ŸáÿßŸàÿ≥',\n",
       " 'ÿ®ŸÑŸÜÿØ',\n",
       " 'ÿØÿßÿ±ŸÉ',\n",
       " 'ÿ¥ŸàŸÉŸàŸÑÿßÿ™ÿ©',\n",
       " 'ÿ®ÿ¥ÿ∞ÿ±ÿßÿ™',\n",
       " 'ÿßŸÑŸÉÿßŸÅŸä',\n",
       " 'ÿ®ÿØŸàŸÜ',\n",
       " 'ÿ≥ŸÉÿ±',\n",
       " 'ŸÖÿ∂ÿßŸÅ',\n",
       " '75',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ŸÅŸä',\n",
       " 'ÿ≥ÿ™ŸäŸÜ',\n",
       " 'ÿØÿßŸáŸäÿ©',\n",
       " 'Stickers',\n",
       " '-',\n",
       " 'Among',\n",
       " 'us',\n",
       " 'ÿ≥ÿßÿπÿ©',\n",
       " 'ÿ®ŸÑŸÇŸäÿ≥',\n",
       " 'ÿßŸÑŸÜÿ≥ÿßÿ¶ŸäŸá',\n",
       " 'ÿ®ÿßŸÑŸÑŸàŸÜ',\n",
       " 'ÿßŸÑŸÅÿ∂Ÿä',\n",
       " 'ÿ®ŸÉÿ¨',\n",
       " 'ÿ±ŸÅÿπ',\n",
       " 'ŸÅŸäÿØŸäŸà',\n",
       " 'ÿßŸÉÿ≥ÿ®ŸÑŸàÿ±',\n",
       " 'ÿ™ŸäŸÉ',\n",
       " 'ÿ™ŸàŸÉ',\n",
       " 'ÿπÿ±ÿ∂',\n",
       " 'ŸáŸäŸÑ',\n",
       " 'ÿ£ÿÆÿ∂ÿ±',\n",
       " 'ÿ£ŸÖÿ±ŸäŸÉŸä',\n",
       " 'ÿ¨ÿßŸÖÿ®Ÿà',\n",
       " 'ŸÅÿßÿÆÿ±',\n",
       " 'ÿ±ŸÇŸÖ',\n",
       " '1',\n",
       " '(ŸÉÿ±ÿ™ŸàŸÜ',\n",
       " '5',\n",
       " 'ŸÉŸäŸÑŸà)',\n",
       " 'ÿ®ÿÆÿµŸÖ',\n",
       " '40%',\n",
       " '+',\n",
       " 'ÿ¥ÿ≠ŸÜ',\n",
       " 'ŸÖÿ¨ÿßŸÜŸä',\n",
       " 'ÿµÿ≠ŸÜ',\n",
       " 'ÿ®ŸÑÿßÿ≥ÿ™ŸäŸÉ',\n",
       " 'ŸÖÿ≥ÿ™ÿ∑ŸäŸÑ',\n",
       " 'ÿ±ŸÇŸÖ',\n",
       " '1/2',\n",
       " '(50',\n",
       " 'ÿ≠ÿ®ÿ©)',\n",
       " 'ÿ™ÿ¥Ÿäÿ≤',\n",
       " 'ŸÉŸäŸÉ',\n",
       " 'ÿ≤Ÿäÿ™',\n",
       " 'ÿßŸÑÿ™ŸäŸÜ',\n",
       " 'ÿßŸÑÿ¥ŸàŸÉŸä',\n",
       " 'ÿßŸÑŸÜŸÇŸä',\n",
       " '100%',\n",
       " '-',\n",
       " 'Prickly',\n",
       " 'pear',\n",
       " 'oil',\n",
       " '8400',\n",
       " 'ÿ¥ÿØÿ©',\n",
       " 'ŸÜÿ¥ÿ≠ŸÜ',\n",
       " 'ÿßŸäÿØŸäŸÉ',\n",
       " 'ÿ™ŸÇÿ≥Ÿäÿ∑',\n",
       " 'ŸÖÿ≥ŸÉ',\n",
       " 'ÿßŸÑÿ®ÿßŸàÿØÿ±',\n",
       " 'ŸÉÿ™',\n",
       " 'ŸÉÿßÿ™',\n",
       " '.ŸÉŸàÿ≤ŸäŸÜ',\n",
       " 'ÿßŸÑÿØÿ¨ÿßÿ¨',\n",
       " 'ÿ∑ÿπÿßŸÖ',\n",
       " 'ÿ¨ÿßŸÅ',\n",
       " 'ŸÑŸÑŸÇÿ∑ÿ∑',\n",
       " '(ŸÑŸÑÿ≠ÿØ',\n",
       " 'ŸÖŸÜ',\n",
       " 'ŸÉÿ±ÿßÿ™',\n",
       " 'ÿßŸÑÿ¥ÿπÿ±)1.2ŸÉÿ∫',\n",
       " 'ÿ¥ŸÜÿ∑ÿ©',\n",
       " 'ŸÉÿ±Ÿàÿ≥',\n",
       " 'ŸÖÿßÿ±ŸÉÿ©',\n",
       " 'ŸÑŸàŸäÿ≥',\n",
       " 'ŸÅŸäÿ™ŸàŸÜ',\n",
       " '-',\n",
       " 'LV',\n",
       " 'ALMA',\n",
       " 'BB',\n",
       " 'Pink',\n",
       " 'hk',\n",
       " 'ŸÖÿ®ÿÆÿ±ÿ©',\n",
       " 'ŸÖŸÑÿßÿ®ÿ≥',\n",
       " 'ÿÆÿ¥ÿ®Ÿäÿ©',\n",
       " 'ŸÜŸÇÿßÿ®',\n",
       " 'Ÿàÿ≥ÿ∑',\n",
       " 'ŸÖŸÇŸàŸâ',\n",
       " 'ŸÑÿµŸÇ',\n",
       " 'ÿ®ÿ±ÿØÿ©',\n",
       " 'ÿ£ŸàŸÇŸÅ',\n",
       " 'Ÿàÿ£ŸÉŸÅŸÑ',\n",
       " 'ŸÜŸÉŸáÿ©',\n",
       " 'ÿ≥ŸàŸÑÿ™',\n",
       " 'ŸÑŸàÿ¥',\n",
       " 'ÿßŸäÿ≥',\n",
       " 'VGOD',\n",
       " 'LUSH',\n",
       " 'ICE',\n",
       " 'ÿ¥ŸÑÿ¥ŸÑŸä',\n",
       " '-',\n",
       " 'ÿ•ÿ´ŸäŸàÿ®Ÿäÿß',\n",
       " '250',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ÿ≠ÿ∞ÿßÿ°',\n",
       " 'ÿ¥ÿ±ŸÇŸä',\n",
       " 'ŸÖÿ∑ÿ±ÿ≤',\n",
       " 'ÿ≥ŸÅÿ±ÿ©',\n",
       " 'ÿ∑ÿπÿßŸÖ',\n",
       " 'ÿ≥ÿßÿØÿ©',\n",
       " '100*110ÿ≥ŸÖ',\n",
       " '(35',\n",
       " 'ÿ≥ŸÅÿ±ÿ©)',\n",
       " 'ÿßŸÑÿπŸÇŸéŸäŸÇ',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '039',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '040',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '041',\n",
       " 'ŸÅÿ≥ÿ™ÿßŸÜ',\n",
       " 'ÿßÿ≥ŸàÿØ',\n",
       " 'ÿ®ÿ¥ÿπÿßÿ±',\n",
       " 'ŸäŸàŸÖ',\n",
       " 'ÿßŸÑÿ™ÿßÿ≥Ÿäÿ≥',\n",
       " 'ŸÇŸÑŸàÿ≥',\n",
       " 'ÿßŸäŸÑŸÅ',\n",
       " 'MOCHA',\n",
       " 'TWIST',\n",
       " 'ÿπÿ®ÿßŸäŸá',\n",
       " 'ŸÉÿ™ÿßŸÜ',\n",
       " 'ÿ®ÿ£ŸÉŸÖÿßŸÖ',\n",
       " 'ÿ™ŸÅÿ™Ÿá',\n",
       " 'ÿ™ÿ±ŸÉŸä',\n",
       " 'ÿØÿ±ÿßÿ¨ÿ©',\n",
       " 'ÿßŸàÿ±ÿ®ÿ™ÿ±ŸÉ',\n",
       " 'ÿßŸÑŸÖÿ∑Ÿàÿ±ÿ©',\n",
       " 'ÿ¥ŸäŸÅÿ±ŸàŸÑŸäÿ©',\n",
       " 'ÿ¨ŸÖÿ≥',\n",
       " 'GMC',\n",
       " 'ÿ™ÿ∂ŸÑŸäŸÑ',\n",
       " 'ŸÇÿßÿ®ŸÑ',\n",
       " 'ŸÑŸÑŸÅŸÉ',\n",
       " 'Ÿà',\n",
       " 'ÿßŸÑÿ™ÿ±ŸÉŸäÿ®',\n",
       " 'ŸÖÿ∑ÿßÿ±ÿ©',\n",
       " 'ŸÖÿßÿ°',\n",
       " 'ÿ®ŸÑŸäÿØ',\n",
       " 'ÿ≥ÿ®Ÿàÿ±ÿ™',\n",
       " 'ÿ¥ŸäŸÉÿ±',\n",
       " '600',\n",
       " 'ŸÖŸÑ',\n",
       " 'Blade',\n",
       " 'Sport',\n",
       " 'ŸÖÿ±ÿ∑ÿ®',\n",
       " 'ÿßŸÑÿ≥ŸÑÿßÿ¥',\n",
       " 'ÿ®ÿ≤ŸÑ',\n",
       " 'ÿßŸÑŸàÿßÿ≠',\n",
       " 'ÿ≥ŸÑÿ≥ÿßŸÑ',\n",
       " 'ÿ≥Ÿàÿßÿ±ŸàŸÅÿ≥ŸÉŸä',\n",
       " 'ÿßÿ≠ÿØÿ´',\n",
       " 'ÿßÿµÿØÿßÿ±',\n",
       " 'ÿØŸäŸäÿ≥',\n",
       " 'ŸÉŸàŸÑÿØ',\n",
       " 'ÿßŸÑÿ¨ÿØŸäÿØ',\n",
       " 'ÿ®',\n",
       " '3',\n",
       " 'ÿπÿØÿ≥ÿßÿ™',\n",
       " 'DEESS',\n",
       " 'COLD',\n",
       " 'GP591',\n",
       " 'ÿ∑ÿßŸàŸÑÿ©',\n",
       " 'ÿÆÿØŸÖÿ©',\n",
       " 'ŸÇÿßÿπÿØÿ©',\n",
       " 'ÿ≠ÿØŸäÿØ',\n",
       " 'Ÿàÿ≥ÿ∑ÿ≠',\n",
       " 'ÿπŸÑŸâ',\n",
       " 'ÿ¥ŸÉŸÑ',\n",
       " 'ÿÆÿ¥ÿ®Ÿä',\n",
       " 'ÿ¨ÿ≤ŸÖÿ©',\n",
       " 'ŸÜÿ≥ÿßÿ¶Ÿäÿ©',\n",
       " 'ŸÖŸÜ',\n",
       " 'ŸÖÿßÿ±ŸÉÿ©',\n",
       " 'ÿ™Ÿàÿ±Ÿä',\n",
       " 'ÿ®Ÿàÿ±ÿ¥',\n",
       " 'ŸÖÿ¨ŸÖŸàÿπÿ©',\n",
       " 'ÿ¨ŸÑ',\n",
       " 'ÿßŸÑÿµÿ®ÿßÿ±',\n",
       " 'RDA',\n",
       " 'ÿßÿ≠ÿ™ÿßÿ¨',\n",
       " 'ÿ¨ÿ±ÿπÿ©',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿßŸÑŸÉÿ™ÿ®',\n",
       " 'ÿ≥ÿ™ŸäŸÉÿ±',\n",
       " 'ÿ¥ŸÖÿßŸÑŸäŸÄ/ŸÄÿ©',\n",
       " 'ŸÉÿ™ÿßÿ®',\n",
       " 'ÿßÿ≥ÿ±ÿßÿ±',\n",
       " 'ÿßŸÑŸÉŸàŸÉŸäÿ≤',\n",
       " 'ÿßŸÑÿÆŸÅŸäÿ©',\n",
       " 'ŸÉÿ®ŸÉ',\n",
       " 'ÿ®ÿßŸÑÿßÿ≥ŸÖ',\n",
       " 'ŸÅÿÆÿßŸÖŸá',\n",
       " 'ÿ®ÿ∑ŸÑÿßÿ°',\n",
       " 'ÿßŸÑŸÅÿ∂ÿ©',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '059',\n",
       " 'ŸÉŸÅÿ±',\n",
       " 'ÿ¨ŸàÿßŸÑ',\n",
       " 'ŸÇŸàŸäÿßÿ±ÿØ',\n",
       " 'ÿ£ÿµŸÅÿ±',\n",
       " 'ŸÖÿπ',\n",
       " 'ÿ±ÿ≥ŸÖÿ©',\n",
       " 'Ÿà',\n",
       " 'ÿ¨Ÿäÿ®',\n",
       " 'ŸÑŸÑÿ®ÿ∑ÿßŸÇÿßÿ™',\n",
       " 'ŸÜÿ™',\n",
       " 'ŸÅŸÑŸÉÿ≥',\n",
       " '(',\n",
       " 'ÿ∂ŸÖÿßŸÜ',\n",
       " 'ÿ™ÿ¥ÿ∫ŸäŸÑŸä',\n",
       " ')',\n",
       " 'ŸäŸàŸÑÿßŸÜÿØ',\n",
       " 'ÿ®ŸäŸàÿ™Ÿä',\n",
       " '-',\n",
       " 'ÿßÿ≠ŸÖÿ±',\n",
       " 'ÿ¥ŸÅÿßŸá',\n",
       " 'ŸÖÿ∑ŸÅŸä',\n",
       " '27',\n",
       " 'ÿßŸÑŸÅÿ≠ŸÖ',\n",
       " 'ÿßŸÑÿ∞Ÿáÿ®Ÿä2*5',\n",
       " 'ÿßÿ≥ŸÖŸÉ',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿ∞Ÿáÿ®',\n",
       " 'ŸÑÿµŸÇÿßÿ™',\n",
       " 'ÿßŸÑÿ£ÿπÿ¥ÿßÿ®',\n",
       " 'ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©',\n",
       " 'ŸÑÿ•ÿÆŸÅÿßÿ°',\n",
       " 'ÿßŸÑÿ≠ÿ®Ÿàÿ®',\n",
       " 'ŸàÿßŸÑÿ¢ÿ´ÿßÿ±',\n",
       " 'ŸÖŸÜ',\n",
       " 'ŸÜÿßÿ≥ŸäŸÅŸäŸÉ',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '066',\n",
       " 'ÿπŸÑÿ®ÿ©',\n",
       " 'ÿ¥Ÿäÿ≤ÿ±',\n",
       " '-',\n",
       " 'ÿØÿ¨ÿßÿ¨',\n",
       " 'ŸàÿßŸÑŸÑÿ≠ŸÖ',\n",
       " 'ÿßŸÑÿ®ŸÇÿ±Ÿä',\n",
       " 'ŸÖÿπ',\n",
       " 'ÿßŸÑÿ£ÿ±ÿ≤',\n",
       " 'Ÿàÿ¨ÿ®ÿ©',\n",
       " 'ÿ∑ÿ®ŸäÿπŸäÿ©',\n",
       " '85',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ÿ™Ÿäÿ¥Ÿäÿ±ÿ™',\n",
       " 'ÿ™ÿØÿ±Ÿäÿ®',\n",
       " 'ÿßÿ®Ÿäÿ∂',\n",
       " '$25',\n",
       " 'ŸÖÿ¨ŸàŸáÿ±ÿ©',\n",
       " 'ŸäŸÑÿß',\n",
       " 'ŸÑŸàÿØŸà',\n",
       " 'ŸÉŸÑŸàŸä',\n",
       " 'ÿ≠ŸÇŸäÿ®ÿ©',\n",
       " 'ÿµÿ∫Ÿäÿ±ÿ©',\n",
       " 'ÿÆÿ¥ÿ®Ÿäÿ©',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿßŸÑŸÇŸÖÿßÿ¥',\n",
       " 'ÿßŸÑŸÇÿ∑ŸÜŸä',\n",
       " 'ŸÉŸàŸÅŸÑÿ©',\n",
       " 'ŸÖŸàÿßŸÑŸäÿØ',\n",
       " 'ÿ≠ÿ∞ÿßÿ°',\n",
       " 'ÿ¥ÿ±ŸÇŸä',\n",
       " 'ŸÖÿ∑ÿ±ÿ≤',\n",
       " 'ŸÇŸàÿ™ÿ¥Ÿä',\n",
       " 'ŸÅŸÑŸàÿ±ÿß',\n",
       " '1996',\n",
       " 'ÿ®ŸàŸÉÿ≥',\n",
       " 'ŸÉŸäÿ±Ÿä',\n",
       " 'ÿ±Ÿàÿ≤',\n",
       " 'ÿßŸÑŸÅÿßÿ≤Ÿá',\n",
       " 'ÿßŸÑÿ•ŸÅÿ±ŸäŸÇŸäŸá',\n",
       " 'ÿ™ÿ≠ŸàŸäŸÑ',\n",
       " 'ÿßŸä',\n",
       " 'ÿµŸàÿ±ÿ©',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿπŸÜÿØŸÉ',\n",
       " 'ÿßŸÑŸâ',\n",
       " 'ÿ®Ÿàÿ≥ÿ™ÿ±',\n",
       " '-',\n",
       " 'ÿ®Ÿàÿ≥ÿ™ÿ±',\n",
       " 'ÿ∑ŸÑÿ®',\n",
       " 'ÿÆÿßÿµ',\n",
       " 'Blue',\n",
       " 'ÿ±ŸäÿßŸÑ',\n",
       " 'ÿπÿ±ÿ®Ÿä',\n",
       " 'ÿ≥ÿπŸàÿØŸä',\n",
       " 'ŸÅÿ∂ÿ©',\n",
       " '11.6',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ÿπŸäÿßÿ±',\n",
       " '917',\n",
       " 'ŸÖŸÇÿ¥ÿ±',\n",
       " 'ÿßÿ≠ŸÖÿßÿ∂',\n",
       " 'ŸÇŸàŸä',\n",
       " 'ÿ™ÿ±ŸÉŸäÿ≤',\n",
       " '60',\n",
       " 'ŸÑŸäÿ®ÿ±',\n",
       " '(R-23)',\n",
       " 'ÿ®ŸÉÿ¨',\n",
       " 'Plus',\n",
       " '(ÿ¥ÿ≠ŸÜ',\n",
       " 'ŸÖÿ¨ÿßŸÜŸä)',\n",
       " 'ÿ®Ÿáÿßÿ±ÿßÿ™',\n",
       " 'ÿßŸÑÿ¥Ÿàÿ±ÿ®ÿ©',\n",
       " '90',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '083',\n",
       " 'R069100',\n",
       " 'ŸÉŸàŸÜÿ≥ŸàŸÑ',\n",
       " 'ÿ≠ÿ∞ÿßÿ°',\n",
       " 'ÿ¥ÿ±ŸÇŸä',\n",
       " 'ŸÖÿ∑ÿ±ÿ≤',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿ®ÿßÿ™ŸàŸÜŸäÿß',\n",
       " 'ŸÅÿ≥ÿ™ÿßŸÜ',\n",
       " 'ÿ≥Ÿáÿ±Ÿá',\n",
       " 'ŸÜÿßÿπŸÖ',\n",
       " 'ÿ®ÿßŸÑŸäÿ™',\n",
       " 'ŸÉŸàŸÜÿ™Ÿàÿ±',\n",
       " 'ÿßŸàŸÅŸäÿ≥',\n",
       " '1',\n",
       " 'Converse',\n",
       " 'ŸÖÿ¨ŸÖŸàÿπÿ©',\n",
       " 'ŸÉÿ±ŸäŸÖ',\n",
       " 'ÿ™ŸÅÿ™Ÿäÿ≠',\n",
       " 'ÿßŸÑŸÖŸÜÿßÿ∑ŸÇ',\n",
       " 'ÿßŸÑÿ≠ÿ≥ÿßÿ≥ÿ©',\n",
       " '|',\n",
       " 'ŸàÿßŸÑÿ±ŸÉÿ®',\n",
       " 'ŸàÿßŸÑÿßŸÉŸàÿßÿπ',\n",
       " 'ÿßÿ¥ÿ™ÿ±ÿßŸÉ',\n",
       " 'ÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™',\n",
       " 'ÿßŸÑÿ®ŸÑÿ≥',\n",
       " 'ŸÑŸÖÿØŸá',\n",
       " 'ÿ≥ŸÜŸá',\n",
       " 'ŸÉÿßŸÖŸÑŸá',\n",
       " 'ÿπÿ®ÿßŸäÿ©',\n",
       " 'ŸÉŸÑŸàÿ¥',\n",
       " 'ÿ≥ŸàÿØÿßÿ°',\n",
       " 'ÿ®ŸÇÿµÿ©',\n",
       " 'ŸÖŸÖŸäÿ≤ÿ©',\n",
       " '1196',\n",
       " 'ÿ≥ÿ™ŸäŸÉÿ±',\n",
       " 'accepted',\n",
       " 'my',\n",
       " 'mistake',\n",
       " 'ŸÜÿ¥ÿ≠ŸÜ',\n",
       " 'ŸÑŸÉ',\n",
       " '340',\n",
       " 'ÿ¥ÿØÿ©',\n",
       " 'ÿ¥ŸÖÿßÿ∫',\n",
       " 'Velvet',\n",
       " 'platinum2023',\n",
       " 'ÿ≥ÿßÿπÿ©',\n",
       " 'ÿ£ÿ±ÿ¨ŸàÿßŸÜ',\n",
       " 'ÿ£ŸÑÿ™ÿ±ÿß',\n",
       " 'ÿßŸÑÿ±Ÿäÿßÿ∂ÿ©',\n",
       " 'ÿ¥ÿßŸáÿØVIP',\n",
       " 'ÿ≥ŸÜÿ©',\n",
       " 'ŸàŸäÿ®ŸÉŸà',\n",
       " '-',\n",
       " 'ÿµÿßÿ®ŸàŸÜÿ©',\n",
       " 'ÿßŸÑÿ≠Ÿàÿßÿ¨ÿ®',\n",
       " '25',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '098',\n",
       " 'ÿßÿ¥ÿ™ÿ±ÿßŸÉ',\n",
       " 'ŸáŸàŸÑŸÉ',\n",
       " 'ÿ´ŸÑÿßÿ´',\n",
       " 'ÿ¥ŸáŸàÿ±',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '100',\n",
       " 'ÿ±ŸäŸÅ',\n",
       " '11',\n",
       " 'ÿπÿØÿ≥ÿ©',\n",
       " 'ÿßÿ≤ÿßŸÑÿ©',\n",
       " 'ÿßŸÑÿ¥ÿπÿ±',\n",
       " '500',\n",
       " 'ÿßŸÑŸÅ',\n",
       " 'ŸàŸÖÿ∂ÿ©',\n",
       " 'ÿ≥ŸÉŸäÿ™ÿ¥Ÿäÿ±ÿ≤',\n",
       " 'ÿ≠ÿ∞ÿßÿ°',\n",
       " 'ÿ±Ÿäÿßÿ∂Ÿä',\n",
       " 'ŸÜÿ≥ÿßÿ¶Ÿä',\n",
       " '67/',\n",
       " 'ÿ≠ÿßŸÑÿ©',\n",
       " 'ÿ±ŸÇŸÖ',\n",
       " '(',\n",
       " '21667',\n",
       " ')',\n",
       " ':',\n",
       " 'ÿ≠ÿßŸÑÿ©',\n",
       " 'ÿ∑ŸÅŸÑÿ©',\n",
       " 'ÿ®ÿ∞Ÿàÿ±',\n",
       " 'ŸÜÿπŸÜÿßÿπ',\n",
       " 'ÿßÿ≥ÿ®ÿßŸÜŸä',\n",
       " 'ÿ∞Ÿà',\n",
       " 'ÿ±ÿßÿ¶ÿ≠ÿ©',\n",
       " 'ŸàŸÜŸÉŸáÿ©',\n",
       " 'ŸÇŸàŸäÿ©',\n",
       " 'ÿ¨ÿØÿß',\n",
       " 'ÿ®ÿßŸÉŸäÿ™',\n",
       " '0.5',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ÿ¨ÿ±ÿ¨Ÿäÿ±',\n",
       " 'ÿ∑ÿßÿ≤ÿ¨',\n",
       " 'ÿ±ÿ®ÿ∑ÿ©',\n",
       " 'ŸÖŸÇÿ≥ŸÖ',\n",
       " 'ÿµÿ∫Ÿäÿ±',\n",
       " 'ŸÇÿßÿ®ŸÑ',\n",
       " 'ŸÑŸÑÿ™ŸÖÿØÿØ',\n",
       " '-',\n",
       " '6',\n",
       " 'ŸÇÿ∑ÿπ',\n",
       " 'ÿπÿ±ÿ∂',\n",
       " 'ÿßŸÑŸÄŸÄŸÄŸÄ29',\n",
       " 'ŸÖÿ¨ŸÖŸàÿπÿ©',\n",
       " 'ÿ™ÿ®ŸäŸäÿ∂',\n",
       " 'ÿßŸÑÿ£ÿ≥ŸÜÿßŸÜ',\n",
       " 'ÿ®ŸäŸÑŸäÿ≤ÿß',\n",
       " '|',\n",
       " '4',\n",
       " 'ÿßŸÇŸÑÿßŸÖ',\n",
       " 'ÿ¨ŸÑ',\n",
       " '-',\n",
       " 'ŸÖÿ¨ÿßŸÜÿß',\n",
       " '+',\n",
       " 'ŸÖŸÜÿ™ÿ¨',\n",
       " 'ÿßÿ∂ÿßŸÅŸä',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿßÿÆÿ™Ÿäÿßÿ±ŸÉ',\n",
       " 'ÿßŸÑŸÖÿπÿßÿµÿ±',\n",
       " 'ŸÖÿßÿ´',\n",
       " 'ÿµŸÅ',\n",
       " 'ÿßÿ®ÿπ',\n",
       " 'ÿßÿ®ÿ™ÿØÿßÿ¶Ÿä',\n",
       " 'ÿ™ÿ±ŸÖ',\n",
       " '2',\n",
       " '2023',\n",
       " 'ÿ™ŸÅÿßÿ≠',\n",
       " 'ÿ£ÿÆÿ∂ÿ±',\n",
       " 'ÿ≠ÿßŸÖÿ∂',\n",
       " 'ÿ≠ŸÑŸà',\n",
       " 'ÿµÿ≠ŸÜ',\n",
       " 'ÿ¥ŸÜÿ∑ÿ©',\n",
       " '/',\n",
       " 'ÿ≠ŸÇŸäÿ®ÿ©',\n",
       " 'ÿ±ÿ≠ŸÑÿßÿ™',\n",
       " 'ÿ¨Ÿáÿßÿ≤',\n",
       " 'ÿßŸÑÿÆŸäÿ∑',\n",
       " 'ÿßŸÑŸÖÿßÿ¶Ÿä',\n",
       " 'ŸÑÿ™ŸÜÿ∏ŸäŸÅ',\n",
       " 'ÿßŸÑÿ£ÿ≥ŸÜÿßŸÜ',\n",
       " 'ÿßŸÑŸÖÿ∑Ÿàÿ±',\n",
       " 'ÿßŸÑÿ≠ÿØŸäÿ´',\n",
       " '-',\n",
       " '6',\n",
       " 'ÿ±ÿ§Ÿàÿ≥',\n",
       " 'ŸÖÿÆÿ™ŸÑŸÅÿ©',\n",
       " 'ŸÖÿ¨ŸÖŸàÿπÿ©',\n",
       " 'LORA',\n",
       " 'New',\n",
       " 'Balance',\n",
       " '550',\n",
       " 'White',\n",
       " 'Green',\n",
       " 'ÿπÿ±ÿ∂',\n",
       " 'ÿ±ŸÖÿ∂ÿßŸÜ',\n",
       " 'ÿ™ÿ¨ŸÖŸäÿπÿ©',\n",
       " 'i5-12400F-RTX3050',\n",
       " 'ÿ®ÿ±Ÿàÿ¥',\n",
       " 'ÿ¥ŸÜÿ∑ÿ©',\n",
       " 'ÿßŸÑÿ≥ŸÅÿ±',\n",
       " 'ŸÅŸÑŸÅŸÑ',\n",
       " 'ÿ£ÿ®Ÿäÿ∂',\n",
       " 'ŸÖÿ∑ÿ≠ŸàŸÜ',\n",
       " 'ŸÅŸäŸàŸÑÿß',\n",
       " 'ÿ¥ÿ®ÿßÿ®',\n",
       " 'ŸÅŸàÿ±Ÿä',\n",
       " 'ŸÑŸÑŸàÿ¨Ÿá',\n",
       " 'ŸàŸÖŸÜÿ∑ŸÇÿ©',\n",
       " 'ÿ≠ŸàŸÑ',\n",
       " 'ÿßŸÑÿπŸäŸÜ',\n",
       " '50',\n",
       " 'ŸÖŸÑ',\n",
       " 'ÿØŸáŸÜ',\n",
       " 'ÿ®ÿ±ÿßÿ¥ŸäŸÜ',\n",
       " 'ŸÅÿßÿÆÿ±',\n",
       " 'ÿπÿ±ÿ∂',\n",
       " '3',\n",
       " 'ŸÖŸÜÿ™ÿ¨ÿßÿ™',\n",
       " 'ÿßŸÑÿßŸäŸÅŸàŸÜ',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿ≤Ÿäÿ™',\n",
       " 'ÿπÿ±ŸÇ',\n",
       " 'ÿßŸÑÿ≥Ÿàÿ≥',\n",
       " 'ÿßŸÑÿπÿ∂ŸàŸä',\n",
       " 'ŸÖŸÜ',\n",
       " 'ŸÜÿßÿ™ÿ¥ÿ±ÿ≤',\n",
       " 'ÿßŸÜÿ≥ÿ±',\n",
       " '30',\n",
       " 'ŸÖŸÑ',\n",
       " 'ŸÖÿ™ÿßÿ®ÿπŸäŸÜ',\n",
       " 'ÿßŸÜÿ≥ÿ™ÿß',\n",
       " 'ŸÖŸÉÿ≥',\n",
       " 'ŸáŸàŸÜÿØÿßŸä',\n",
       " 'ÿ™ÿ∏ŸÑŸäŸÑ',\n",
       " 'ÿ≥ŸáŸÑ',\n",
       " 'ÿßŸÑŸÅŸÉ',\n",
       " 'Ÿà',\n",
       " 'ÿßŸÑÿ™ÿ±ŸÉŸäÿ®',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '125',\n",
       " 'ŸÖÿßŸÉŸäŸÜÿ©',\n",
       " 'ŸÑŸÉÿßŸÖŸÑ',\n",
       " 'ÿßŸÑÿ¨ÿ≥ŸÖ',\n",
       " 'ŸÑŸÑÿ±ÿ¨ÿßŸÑ',\n",
       " 'ŸàÿßŸÑŸÜÿ≥ÿßÿ°',\n",
       " 'ŸÑŸàŸÜ',\n",
       " 'ÿ£ÿ≥ŸàÿØ',\n",
       " 'ÿ±ŸÖŸÑ',\n",
       " 'ŸÉÿ™',\n",
       " 'ŸÉÿßÿ™',\n",
       " 'ÿ®ÿ±ÿßÿ¶ÿ≠ÿ©',\n",
       " 'ÿ®ŸàÿØÿ±ÿ©',\n",
       " 'ÿßŸÑÿßÿ∑ŸÅÿßŸÑ10L',\n",
       " 'ÿ™Ÿäÿ¥Ÿäÿ±ÿ™',\n",
       " 'ÿßÿ≠ŸÖÿßÿ°',\n",
       " 'ÿßŸÑÿ®ÿ±ÿ™ÿ∫ÿßŸÑ',\n",
       " '2022',\n",
       " 'ŸÉŸàÿ®',\n",
       " 'ÿßŸÑÿ∫ŸäŸÖŸá',\n",
       " '2',\n",
       " 'ŸÖŸÜÿßÿØŸäŸÑ',\n",
       " '500',\n",
       " 'ŸÖŸÅÿ±ÿØ',\n",
       " 'ÿ™ŸÅÿßÿ≠',\n",
       " 'ÿ∫ÿ±ÿßŸÜŸä',\n",
       " 'ŸÅÿ±ŸÜÿ≥Ÿä',\n",
       " 'ÿπÿ∂ŸàŸä',\n",
       " '/',\n",
       " '1',\n",
       " 'ŸÉÿ¨ŸÖ',\n",
       " 'ÿ≠ÿßŸÖŸÑ',\n",
       " 'ŸÖÿ∫ŸÜÿßÿ∑Ÿäÿ≥Ÿä',\n",
       " 'Ÿà',\n",
       " 'ŸÖÿ´ÿ®ÿ™',\n",
       " 'ŸÑŸÑŸáŸàÿßÿ™ŸÅ',\n",
       " 'ŸÅŸä',\n",
       " 'ÿßŸÑÿ≥Ÿäÿßÿ±ÿ©',\n",
       " 'ŸÖŸÜ',\n",
       " 'ŸàŸàÿ®Ÿà',\n",
       " '\\u200fÿ¥ÿπÿ±',\n",
       " 'Ÿäÿ≤ŸäÿØ',\n",
       " 'ÿ®ŸÜ',\n",
       " 'ÿßŸÑÿ∑ÿ´ÿ±Ÿäÿ©',\n",
       " 'ÿØÿ±ÿßÿ≥ÿ©',\n",
       " 'Ÿàÿ¨ŸÖÿπ',\n",
       " 'Ÿàÿ™ÿ≠ŸÇŸäŸÇ',\n",
       " '\\u200fÿØ.ŸÜÿßÿµÿ±',\n",
       " 'ÿ®ŸÜ',\n",
       " 'ÿ≥ÿπÿØ',\n",
       " 'ÿßŸÑÿ±ÿ¥ŸäÿØ',\n",
       " '1400ŸáŸÄ',\n",
       " 'ÿ≠Ÿàÿ∂',\n",
       " 'ÿ±ÿ∑Ÿàÿ®ÿ©',\n",
       " 'ÿßŸÑÿ¥ÿßŸÖŸÑÿ©',\n",
       " 'ÿ®ŸÑÿ≥',\n",
       " 'ÿ£ŸÇÿ±ÿßÿµ',\n",
       " 'ÿ®ŸÑÿßŸäÿ≥ÿ™Ÿäÿ¥ŸÜ',\n",
       " '5',\n",
       " '(ÿ•ÿµÿØÿßÿ±',\n",
       " 'ÿßŸÑÿ£ŸÇÿ±ÿßÿµ)',\n",
       " '+',\n",
       " '1',\n",
       " 'ÿ∞ÿ±ÿßÿπ',\n",
       " 'ÿ™ÿ≠ŸÉŸÖ',\n",
       " 'ÿßÿ∂ÿßŸÅŸä',\n",
       " 'ŸÑŸäÿØŸä',\n",
       " 'ÿØŸäŸàÿ±',\n",
       " 'ŸÉÿ±ÿ≥ÿ™ÿßŸÑ',\n",
       " '24',\n",
       " 'ÿµÿßÿ®ŸàŸÜŸá',\n",
       " 'ÿßŸÑŸÅÿ±ÿßÿ¥Ÿá',\n",
       " 'ŸÖÿπ',\n",
       " 'ÿßŸÑÿßÿ≥ŸÖ',\n",
       " '1',\n",
       " 'ŸÖŸÑŸäŸàŸÜ',\n",
       " 'ŸÉŸàŸäŸÜÿ≤',\n",
       " '(',\n",
       " 'ÿ≥ŸàŸÜŸä',\n",
       " '4/5',\n",
       " ')',\n",
       " '+',\n",
       " '1',\n",
       " 'ŸÖŸÑŸäŸàŸÜ',\n",
       " 'ŸÉŸàŸäŸÜÿ≤',\n",
       " 'ÿ≤ŸäÿßÿØÿ©!',\n",
       " 'ÿ≥ŸäŸÅ',\n",
       " 'ÿ≤Ÿàÿ±Ÿà',\n",
       " 'ÿßŸÑÿßÿ≠ŸÖÿ±',\n",
       " 'ÿÆÿ¥ÿ®',\n",
       " '|',\n",
       " 'One',\n",
       " 'Piece',\n",
       " 'Swords',\n",
       " 'ŸÖÿπŸÑÿ®ÿßÿ™',\n",
       " 'ÿØÿ¨ÿßÿ¨',\n",
       " 'ŸàŸÑÿ≠ŸÖ',\n",
       " 'ÿßŸÑÿ∂ÿßŸÜ',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿ¥ÿ±ŸÉÿ©',\n",
       " 'Carnilove',\n",
       " 'ŸÖŸäÿ≤ÿßŸÜ',\n",
       " 'ÿßŸÑŸÉÿ™ÿ±ŸàŸÜŸä',\n",
       " 'ŸÑŸÑŸÇŸáŸàÿ©',\n",
       " 'ŸÖÿ¨ŸÖŸàÿπÿ©',\n",
       " 'ŸÅÿ±ÿ¥',\n",
       " 'ÿ∫ÿ≥ŸäŸÑ',\n",
       " 'ÿπÿ¨ŸÑÿßÿ™',\n",
       " 'ÿßŸÑÿ≥Ÿäÿßÿ±ÿ©',\n",
       " 'ÿ∑ŸÇŸÖ',\n",
       " 'ÿ®ÿßŸÑŸàŸÜÿßÿ™',\n",
       " 'ÿ®ŸÉÿ¨',\n",
       " 'ÿßŸÑŸÉŸäÿ™Ÿà',\n",
       " 'ÿ±Ÿàÿ¨',\n",
       " 'ÿ®ÿØŸäŸÑ',\n",
       " 'ŸÉŸäŸÉŸà',\n",
       " '(',\n",
       " 'ÿ≠ÿ®Ÿá',\n",
       " ')',\n",
       " 'ÿßŸÑŸÖÿ®ÿßÿ±Ÿá',\n",
       " 'ÿßŸÑÿßŸÜŸäŸÇŸá',\n",
       " 'ÿØÿ±ÿßÿ¨ÿ©',\n",
       " 'ŸÜÿßÿ±Ÿäÿ©',\n",
       " '250cc',\n",
       " 'ŸÖŸàÿØŸäŸÑ',\n",
       " '2023',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿ®ÿßŸàÿ±',\n",
       " 'ÿ≥ÿ®Ÿàÿ±ÿ™',\n",
       " 'ÿ®ÿ∑ÿßÿ∑ÿ≥',\n",
       " 'ÿ®ŸäŸàŸÇŸÑÿ≤',\n",
       " 'ÿ¨ÿ®ŸÜ',\n",
       " 'ÿßŸÑŸÜÿßÿ¥Ÿà',\n",
       " 'ÿπÿßÿ¶ŸÑŸä',\n",
       " '125',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'Basic',\n",
       " 'Life',\n",
       " 'Support',\n",
       " '(BLS)English',\n",
       " '&',\n",
       " 'Arabic',\n",
       " 'ÿ®ŸàŸÉÿ≥',\n",
       " 'ÿßŸÑÿ¥ÿ™ÿßÿ°',\n",
       " 'ÿÆÿßÿ™ŸÖ',\n",
       " 'ÿßÿ≤ÿ±ŸÇ',\n",
       " 'CIVIVI',\n",
       " 'Knives',\n",
       " 'Thug',\n",
       " '2',\n",
       " 'ŸÉŸàŸÑŸàŸÖÿ®Ÿäÿß',\n",
       " 'ÿßŸÑ',\n",
       " 'ÿ®ŸÑŸäÿ≤ÿ±',\n",
       " '-',\n",
       " 'ÿßŸÑÿ™ŸàŸÜ',\n",
       " '|',\n",
       " 'Blazer',\n",
       " 'ŸÖÿ¨ŸÖŸàÿπÿ©',\n",
       " 'ÿßŸÑŸàÿ±ÿß',\n",
       " 'ÿßŸÑÿπŸÑÿßÿ¨Ÿäÿ©',\n",
       " 'ŸÖŸÑŸÉÿ©',\n",
       " 'ÿßŸÑŸÉŸÑŸäÿ¨ÿß',\n",
       " 'ŸÖŸäŸÜŸä',\n",
       " 'Ÿàÿ≥ÿ∑',\n",
       " 'ÿßŸÑÿ™ÿ®ŸäÿßŸÜ',\n",
       " 'ŸÅŸä',\n",
       " 'ÿ¢ÿØÿßÿ®',\n",
       " 'ÿ≠ŸÖŸÑÿ©',\n",
       " 'ÿßŸÑŸÇÿ±ÿ¢ŸÜ',\n",
       " 'ŸÖŸÉŸÜÿ≥ÿ©',\n",
       " 'ŸÉŸáÿ±ÿ®ÿßŸäŸîŸäÿ©',\n",
       " 'ŸÑÿßÿ≥ŸÑŸÉŸäÿ©',\n",
       " '023-9-RE',\n",
       " 'ŸÉÿ±ŸäŸÖ',\n",
       " 'ÿπÿ±ŸÇ',\n",
       " 'ÿßŸÑÿ≥Ÿàÿ≥',\n",
       " 'ŸÖŸÜ',\n",
       " 'ŸÜÿßÿ™ÿ¥ÿ±ÿ≤',\n",
       " 'ÿßŸÜÿ≥ÿ±',\n",
       " '60ŸÖŸÑ',\n",
       " 'ÿßÿ¥ÿ™ÿ±ÿßŸÉ',\n",
       " 'ÿ¥ÿßŸáÿØ',\n",
       " 'ŸÑŸÖÿØÿ©',\n",
       " 'ÿ¥Ÿáÿ±',\n",
       " 'vip',\n",
       " 'ŸÖÿµÿßÿµ',\n",
       " 'ÿ¥ŸÅÿßŸÅ',\n",
       " 'ŸÖÿ∫ŸÑŸÅ',\n",
       " '6ŸÖŸÖ',\n",
       " 'ŸÉÿ≤ÿ®ÿ±ÿ©',\n",
       " 'ŸÖÿ∑ÿ≠ŸàŸÜÿ©',\n",
       " '75',\n",
       " 'ÿ¨ÿ±ÿßŸÖ',\n",
       " 'ÿπÿ∂ŸàŸä',\n",
       " '-',\n",
       " 'ÿ£ÿ±ÿ∂',\n",
       " 'ÿßŸÑÿ∑ÿ®Ÿäÿπÿ©',\n",
       " 'ŸÖÿ≥ÿ™ÿ±',\n",
       " 'ÿ≠ŸÖŸàÿ∂ÿ©',\n",
       " 'ROSE',\n",
       " 'EGLANTIRIA',\n",
       " 'ÿ£ŸÉŸäÿßÿ≥',\n",
       " 'ÿ≥ÿßŸÜÿØŸàÿ™ÿ¥',\n",
       " 'ŸÖÿπ',\n",
       " 'ŸÇŸÅŸÑ',\n",
       " 'ŸÑŸÑÿßÿ∑ŸÅÿßŸÑ',\n",
       " 'ÿßŸàÿ±ŸäŸÜŸÉÿ≥',\n",
       " '(50ÿ≠ÿ®ÿ©)',\n",
       " 'ÿ•ŸÅÿ∑ÿßÿ±',\n",
       " 'ÿßŸÑÿ≠ÿ±ŸÖ',\n",
       " '-',\n",
       " 'ŸÖÿ¥ÿ±Ÿàÿπ',\n",
       " 'ÿßŸÅÿ∑ÿßÿ±',\n",
       " 'ÿßŸÑÿµÿßÿ¶ŸÖŸäŸÜ',\n",
       " 'ŸÅŸä',\n",
       " 'ÿ≥ÿßÿ≠ÿßÿ™',\n",
       " 'ÿßŸÑŸÖÿ≥ÿ¨ÿØ',\n",
       " 'ÿßŸÑÿ≠ÿ±ÿßŸÖ',\n",
       " 'ÿ¥ŸÜÿ∑ÿ©',\n",
       " 'ÿ™Ÿàÿ±Ÿä',\n",
       " 'ÿ®Ÿàÿ±ÿ¥',\n",
       " 'ÿ®ÿ®ÿ¨Ÿä',\n",
       " '8,400',\n",
       " 'ÿ¥ÿØÿ©',\n",
       " 'ÿ®ÿ∞Ÿàÿ±',\n",
       " 'ÿßŸÑÿ¨ÿ±ÿ¨Ÿäÿ±',\n",
       " 'ÿßŸÑÿ≠ÿ≥ÿßŸàŸä',\n",
       " 'ÿ®ŸÉÿ¨',\n",
       " 'ÿßŸÑÿ¥ÿ™ÿßÿ°',\n",
       " 'ŸÑÿßÿ≥Ÿäÿß',\n",
       " 'ÿ®ÿßÿ±ŸÅŸàŸÖ',\n",
       " 'ÿ≤Ÿäÿ™',\n",
       " 'ÿ™ÿ¥ÿßŸÜÿ≥',\n",
       " 'ÿ™ŸÜÿØÿ±ÿ¥ÿßŸÜŸäŸÑ',\n",
       " 'ÿ¥ÿßŸáÿØ',\n",
       " 'VIP',\n",
       " 'ÿßŸÑÿ®ÿßŸÇÿ©',\n",
       " 'ÿßŸÑÿπÿßÿØŸäÿ©',\n",
       " '-',\n",
       " 'ÿ¥Ÿáÿ±',\n",
       " 'ÿ¨Ÿáÿßÿ≤',\n",
       " 'ÿßŸÑÿπÿßÿ®',\n",
       " 'ÿßŸÑÿ∑Ÿäÿ®ŸäŸÜ',\n",
       " '10',\n",
       " 'ÿßŸÑÿßŸÅ',\n",
       " 'ŸÑÿπÿ®Ÿá',\n",
       " '64',\n",
       " 'ŸÇŸäŸÇÿß',\n",
       " 'ŸÖÿßŸÉŸäŸÜÿ©',\n",
       " 'ŸÑŸÉÿßŸÖŸÑ',\n",
       " 'ÿßŸÑÿ¨ÿ≥ŸÖ',\n",
       " 'ŸÑŸÑÿ±ÿ¨ÿßŸÑ',\n",
       " 'ŸàÿßŸÑŸÜÿ≥ÿßÿ°',\n",
       " 'ŸÑŸàŸÜ',\n",
       " 'ÿ£ÿ≤ÿ±ŸÇ',\n",
       " 'ÿ∫ÿßŸÖŸÇ',\n",
       " 'ŸÖŸÖŸäÿ≤',\n",
       " 'ÿ≥ÿßÿπÿ©',\n",
       " 'ÿ£ŸÑŸÖÿßÿ≥',\n",
       " 'ŸÜÿ≥ÿßÿ¶Ÿäÿ©',\n",
       " 'ŸÖÿßÿ±ŸÉÿ©',\n",
       " 'ÿ¥ÿßÿ±ŸäŸàŸÑ',\n",
       " 'ÿ≥ŸàŸÅÿßÿ¨',\n",
       " 'ÿØŸäŸàÿ±',\n",
       " 'ÿßŸÑÿ±ÿ¨ÿßŸÑŸä',\n",
       " 'sova‚Äôd',\n",
       " 'ÿπÿ±ÿ∂',\n",
       " 'ÿßŸÑŸÄŸÄŸÄŸÄ29',\n",
       " 'ÿ¨Ÿáÿßÿ≤',\n",
       " 'ÿßŸÑÿÆŸäÿ∑',\n",
       " 'ÿßŸÑŸÖÿßÿ¶Ÿä',\n",
       " 'ŸÑÿ™ŸÜÿ∏ŸäŸÅ',\n",
       " 'ÿßŸÑÿ£ÿ≥ŸÜÿßŸÜ',\n",
       " 'ÿßŸÑŸÖÿ∑Ÿàÿ±',\n",
       " '-',\n",
       " '6',\n",
       " 'ÿ±ÿ§Ÿàÿ≥',\n",
       " 'ŸÖÿÆÿ™ŸÑŸÅÿ©',\n",
       " '+',\n",
       " 'ŸÖŸÜÿ™ÿ¨',\n",
       " 'ÿßÿ∂ÿßŸÅŸâ',\n",
       " 'ŸÖŸÜ',\n",
       " 'ÿßÿÆÿ™Ÿäÿßÿ±ŸÉ',\n",
       " 'ÿ≠ÿ≥ÿßÿ®ÿßÿ™',\n",
       " 'ŸÜÿ™ŸÅŸÑŸÉÿ≥',\n",
       " 'ÿπÿ¥Ÿàÿßÿ¶Ÿä(ÿ∂ŸÖÿßŸÜ',\n",
       " 'ÿ™ÿ¥ÿ∫ŸäŸÑŸä)',\n",
       " 'ÿ™ÿßŸÉŸäÿ≥',\n",
       " 'ÿ®ŸÑŸà',\n",
       " 'ÿßŸÑÿßÿ≤ÿ±ŸÇ',\n",
       " 'ŸÖŸÜÿ™ÿ¨',\n",
       " 'ÿ≠ÿµÿ±Ÿä',\n",
       " 'ŸÖÿ≥ÿ™Ÿàÿ±ÿØ',\n",
       " 'ŸÖŸÉÿ≥ŸäŸÉŸä',\n",
       " 'ÿßŸÑÿ≠ÿ¨ŸÖ',\n",
       " 'ÿßŸÑŸÉÿ®Ÿäÿ±',\n",
       " '113.4',\n",
       " 'ÿ¨ŸÖ',\n",
       " '(ÿßÿ∂ÿßŸÅÿ©)',\n",
       " 'ŸÉŸàÿØ',\n",
       " 'QR',\n",
       " 'ÿßÿ∫ŸÜŸäÿ©',\n",
       " 'ŸÅÿ≥ÿ™ÿßŸÜ',\n",
       " 'ŸÖŸàÿ±ÿØ',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '180',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '181',\n",
       " 'ÿ®ŸÉÿ¨',\n",
       " 'ÿ±ŸÖÿ∂ÿßŸÜ',\n",
       " '#2',\n",
       " '-',\n",
       " 'ŸÑŸÑŸÖŸÇÿßÿ≥ÿßÿ™',\n",
       " 'ÿßŸÑŸÖŸÅÿ±ÿØÿ©',\n",
       " 'ÿßŸÑŸÖÿ±ÿ¢ÿ©',\n",
       " 'ÿßŸÑÿ∞ŸÉŸäÿ©',\n",
       " 'ŸÖÿ±ÿßŸäÿß',\n",
       " 'ÿ∞ŸÉŸäÿ©',\n",
       " 'ŸÖÿπ',\n",
       " 'ÿ¥ÿßÿ≠ŸÜ',\n",
       " 'Ÿàÿ≥ŸÖÿßÿπÿßÿ™',\n",
       " 'ÿ®ŸÑŸàÿ™Ÿàÿ´',\n",
       " 'Ÿàÿ•ÿ∂ÿßÿ°ÿ©',\n",
       " 'LED',\n",
       " 'ÿ™ÿπŸÑŸäŸÇÿ©',\n",
       " 'ÿ¨ŸàÿßŸÑ',\n",
       " 'ŸÖŸÑŸàŸÜÿ©',\n",
       " 'ŸÖÿ´ÿ®ÿ™',\n",
       " 'ŸÖŸÉŸäÿßÿ¨',\n",
       " '-',\n",
       " 'Fixer',\n",
       " 'spray',\n",
       " 'ŸÉÿ±Ÿäÿ≥ÿ™ŸÑ',\n",
       " 'Ÿáÿßÿ±ÿ™ÿ≤',\n",
       " '-',\n",
       " 'ÿßŸÑÿßŸÜŸä',\n",
       " '312',\n",
       " 'ÿ≥ÿØŸäÿ±Ÿä',\n",
       " 'ŸÅŸÜÿØŸä',\n",
       " '5400',\n",
       " 'ÿ¨ŸàŸáÿ±Ÿá-ÿ≠ÿ≥ÿßÿ®',\n",
       " 'ŸÖÿ¥ÿ≠ŸàŸÜ',\n",
       " 'Ÿàÿ¨ÿßŸáÿ≤',\n",
       " 'ŸÖŸáÿßÿ±ÿßÿ™',\n",
       " 'ÿßŸÑÿßÿ™ÿµÿßŸÑM',\n",
       " 'ÿπÿ∑ÿ±',\n",
       " '190',\n",
       " 'ÿßŸäŸÅŸàŸÜ',\n",
       " '14',\n",
       " 'ÿ®ÿ±Ÿà',\n",
       " 'ŸÖÿßŸÉÿ≥',\n",
       " '5G',\n",
       " 'ÿ¨Ÿäÿ¨ÿß',\n",
       " '256',\n",
       " 'ÿ®ŸÜŸÅÿ≥ÿ¨Ÿä',\n",
       " 'ÿ∫ÿßŸÖŸÇ',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a676802",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ba9efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  count\n",
      "29        -  65937\n",
      "59       ŸÖŸÜ  42317\n",
      "10      ÿπÿ∑ÿ±  37643\n",
      "137       +  16048\n",
      "45       ŸÖÿπ  14695\n",
      "397       |  14158\n",
      "303       (  12713\n",
      "306       )  12479\n",
      "32    ÿπÿ®ÿßŸäÿ©  11959\n",
      "244      ŸÖŸÑ  11802\n",
      "611     ÿ∑ŸÇŸÖ  10580\n",
      "108    ÿ¨ÿ±ÿßŸÖ  10200\n",
      "124     ÿπÿ±ÿ∂  10089\n",
      "393    ŸÉÿ±ŸäŸÖ   9945\n",
      "119     ÿ®ŸÉÿ¨   8883\n",
      "275  ŸÖÿ¨ŸÖŸàÿπÿ©   8768\n",
      "83     ÿßÿ≥ŸàÿØ   8585\n",
      "259       3   8544\n",
      "21     ÿ≥ÿßÿπÿ©   7798\n",
      "5         2   7794\n",
      "Total unique words: 113400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "word_counts_df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
    "\n",
    "# Sort by count descending\n",
    "word_counts_df = word_counts_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "print(word_counts_df.head(20))  # top 20 words\n",
    "print(f\"Total unique words: {len(word_counts_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d317f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üßº CLEANING PRODUCT NAMES (Arabic + Units) - FUNCTIONS\n",
      "======================================================================\n",
      "\n",
      "üìä Top 20 frequent cleaned words:\n",
      "       word  count\n",
      "8       ÿπÿ∑ÿ±  37643\n",
      "211      ŸÖŸÑ  21133\n",
      "91     ÿ¨ÿ±ÿßŸÖ  12325\n",
      "26    ÿπÿ®ÿßŸäÿ©  11981\n",
      "542     ÿ∑ŸÇŸÖ  10629\n",
      "106     ÿπÿ±ÿ∂  10369\n",
      "342    ŸÉÿ±ŸäŸÖ  10012\n",
      "114    ŸÉŸäŸÑŸà   9407\n",
      "101     ÿ®ŸÉÿ¨   8983\n",
      "70     ÿßÿ≥ŸàÿØ   8966\n",
      "241  ŸÖÿ¨ŸÖŸàÿπÿ©   8806\n",
      "17     ÿ≥ÿßÿπÿ©   7837\n",
      "348  ÿßÿ¥ÿ™ÿ±ÿßŸÉ   7681\n",
      "41        G   7367\n",
      "470     ŸÑŸàŸÜ   7315\n",
      "686   ÿßŸäŸÅŸàŸÜ   7237\n",
      "34     ÿ¨Ÿáÿßÿ≤   6797\n",
      "131     ÿ¥ÿØÿ©   6020\n",
      "592     ÿ¥Ÿáÿ±   6002\n",
      "121     ÿ≠ÿ®ÿ©   5887\n",
      "\n",
      "üì¶ Unit counts found:\n",
      "{'ŸÖŸÑ': 13647, 'ŸÉŸäŸÑŸà': 7431}\n",
      "\n",
      "üìù Sample cleaned_text & clean_words:\n",
      "                                        product_name  \\\n",
      "0    ÿ≥ÿßÿ¶ŸÑ ÿ∫ÿ≥ŸäŸÑ ŸÑŸÑŸÖŸÑÿßÿ®ÿ≥ ÿ∑ÿ®ŸäÿπŸä ÿπÿØÿØ 2 ÿπÿ®Ÿàÿ© ÿ®ÿ≠ÿ¨ŸÖ 1.8 ŸÑÿ™ÿ±   \n",
      "1                                            ÿπÿ∑ÿ± 002   \n",
      "2  ÿ±Ÿäÿ¨ŸÑŸäÿ≤ ÿ®Ÿàÿ®ÿß ÿ®Ÿàÿ®ÿß ÿ¥ÿ±Ÿäÿ∑ ŸÑÿ®ÿßŸÜ ÿπŸÑŸÉÿ© ÿ®ŸÜŸÉŸáÿ© ÿßŸÑŸÅÿ±ÿßŸàŸÑÿ©...   \n",
      "3                               ÿ≥ÿßÿπÿ© ŸäÿØ ÿ±ÿ¨ÿßŸÑŸäÿ© M34-2   \n",
      "4                ŸÅŸàÿ∑ ÿ™ŸÜÿ∏ŸäŸÅ ŸÖŸäŸÉÿ±ŸàŸÅÿßŸäÿ®ÿ± ÿ±ŸäŸÉÿ≥Ÿà - 6 ÿ≠ÿ®ÿßÿ™   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0    ÿ≥ÿßÿ¶ŸÑ ÿ∫ÿ≥ŸäŸÑ ŸÑŸÑŸÖŸÑÿßÿ®ÿ≥ ÿ∑ÿ®ŸäÿπŸä ÿπÿØÿØ 2 ÿπÿ®Ÿàÿ© ÿ®ÿ≠ÿ¨ŸÖ 1 8 ŸÑÿ™ÿ±   \n",
      "1                                            ÿπÿ∑ÿ± 002   \n",
      "2  ÿ±Ÿäÿ¨ŸÑŸäÿ≤ ÿ®Ÿàÿ®ÿß ÿ®Ÿàÿ®ÿß ÿ¥ÿ±Ÿäÿ∑ ŸÑÿ®ÿßŸÜ ÿπŸÑŸÉÿ© ÿ®ŸÜŸÉŸáÿ© ÿßŸÑŸÅÿ±ÿßŸàŸÑÿ©...   \n",
      "3                               ÿ≥ÿßÿπÿ© ŸäÿØ ÿ±ÿ¨ÿßŸÑŸäÿ© M34 2   \n",
      "4                  ŸÅŸàÿ∑ ÿ™ŸÜÿ∏ŸäŸÅ ŸÖŸäŸÉÿ±ŸàŸÅÿßŸäÿ®ÿ± ÿ±ŸäŸÉÿ≥Ÿà 6 ÿ≠ÿ®ÿßÿ™   \n",
      "\n",
      "                                         clean_words  \n",
      "0  [ÿ≥ÿßÿ¶ŸÑ, ÿ∫ÿ≥ŸäŸÑ, ŸÑŸÑŸÖŸÑÿßÿ®ÿ≥, ÿ∑ÿ®ŸäÿπŸä, ÿπÿØÿØ, 2, ÿπÿ®Ÿàÿ©, ÿ®ÿ≠ÿ¨...  \n",
      "1                                         [ÿπÿ∑ÿ±, 002]  \n",
      "2  [ÿ±Ÿäÿ¨ŸÑŸäÿ≤, ÿ®Ÿàÿ®ÿß, ÿ®Ÿàÿ®ÿß, ÿ¥ÿ±Ÿäÿ∑, ŸÑÿ®ÿßŸÜ, ÿπŸÑŸÉÿ©, ÿ®ŸÜŸÉŸáÿ©, ...  \n",
      "3                         [ÿ≥ÿßÿπÿ©, ŸäÿØ, ÿ±ÿ¨ÿßŸÑŸäÿ©, M34, 2]  \n",
      "4           [ŸÅŸàÿ∑, ÿ™ŸÜÿ∏ŸäŸÅ, ŸÖŸäŸÉÿ±ŸàŸÅÿßŸäÿ®ÿ±, ÿ±ŸäŸÉÿ≥Ÿà, 6, ÿ≠ÿ®ÿßÿ™]  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2.5: CLEAN & PREPARE PRODUCT NAMES (Arabic + Units) - FUNCTIONAL\n",
    "# ============================================================\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üßº CLEANING PRODUCT NAMES (Arabic + Units) - FUNCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ------------------------------\n",
    "# Arabic stopwords (expand anytime)\n",
    "# ------------------------------\n",
    "arabic_stopwords = {\n",
    "    \"ŸÖŸÜ\", \"ŸÖÿπ\", \"ŸÅŸä\", \"ÿπŸÑŸâ\", \"Ÿà\", \"ÿßŸÑŸâ\", \"ÿπŸÜ\", \"Ÿáÿ∞ÿß\", \"ÿ∞ŸÑŸÉ\", \n",
    "    \"ÿßŸà\", \"ÿßŸä\", \"ŸÉŸÑ\", \"ÿ´ŸÖ\", \"ŸáŸà\", \"ŸáŸä\"\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# Function 1: Normalize units (all variations ‚Üí one standard)\n",
    "# ============================================================\n",
    "def normalize_units(text):\n",
    "    text = str(text)\n",
    "    # Normalize ML / ŸÖŸÑ\n",
    "    ml_patterns = [r\"\\bML\\b\", r\"\\bMl\\b\", r\"\\bml\\b\", r\"\\bŸÖŸÑ\\b\", r\"\\bŸÖŸÑŸä\\b\", r\"\\bŸÖŸÑŸäŸÑÿ™ÿ±\\b\"]\n",
    "    for pat in ml_patterns:\n",
    "        text = re.sub(pat, \" ŸÖŸÑ \", text, flags=re.IGNORECASE)\n",
    "    # Normalize KG / ŸÉŸäŸÑŸà\n",
    "    kg_patterns = [r\"\\bKG\\b\", r\"\\bKg\\b\", r\"\\bkg\\b\", r\"\\bŸÉŸäŸÑŸà\\b\", r\"\\bŸÉÿ∫\\b\"]\n",
    "    for pat in kg_patterns:\n",
    "        text = re.sub(pat, \" ŸÉŸäŸÑŸà \", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# ============================================================\n",
    "# Function 2: Clean product name (remove stopwords, symbols, numbers)\n",
    "# ============================================================\n",
    "def clean_product_name(name, stopwords=arabic_stopwords):\n",
    "    name = str(name)\n",
    "    # Apply unit normalization\n",
    "    name = normalize_units(name)\n",
    "    # Keep only Arabic, English letters, and spaces\n",
    "    name = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \" \", name)\n",
    "    # Remove numbers\n",
    "    name = re.sub(r\"\\d+\", \" \", name)\n",
    "    # Normalize multiple spaces\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    # Tokenize\n",
    "    words = name.split()\n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    return words\n",
    "\n",
    "# ============================================================\n",
    "# Function 3: Count word frequencies\n",
    "# ============================================================\n",
    "def count_cleaned_words(df, column='product_name'):\n",
    "    df['clean_words'] = df[column].astype(str).apply(clean_product_name)\n",
    "    all_words = [word for words in df['clean_words'] for word in words]\n",
    "    counts = Counter(all_words)\n",
    "    counts_df = pd.DataFrame(counts.items(), columns=['word', 'count']).sort_values(by='count', ascending=False)\n",
    "    return counts_df\n",
    "\n",
    "# ============================================================\n",
    "# Function 4: Detect units (after normalization)\n",
    "# ============================================================\n",
    "def detect_units(df, column='product_name'):\n",
    "    series = df[column].astype(str).apply(normalize_units)\n",
    "    units = ['ŸÖŸÑ', 'ŸÉŸäŸÑŸà']  # normalized units\n",
    "    unit_counts = {unit: int(series.str.count(fr\"\\b{unit}\\b\").sum()) for unit in units}\n",
    "    return unit_counts\n",
    "\n",
    "# ============================================================\n",
    "# Function 5: Normalize and clean in-place (update clean_words & cleaned_text)\n",
    "# ============================================================\n",
    "def normalize_and_clean_simple(df, column='product_name'):\n",
    "    for idx, text in df[column].astype(str).items():\n",
    "        text = normalize_units(text)\n",
    "        # Clean text\n",
    "        text_clean = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \" \", text)\n",
    "        text_clean = re.sub(r\"\\s+\", \" \", text_clean).strip()\n",
    "        words = [w for w in text_clean.split() if w not in arabic_stopwords]\n",
    "        # Update columns in-place\n",
    "        df.at[idx, 'clean_words'] = words\n",
    "        df.at[idx, 'cleaned_text'] = \" \".join(words)\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# USAGE EXAMPLES:\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Count cleaned words\n",
    "cleaned_word_counts_df = count_cleaned_words(df)\n",
    "print(\"\\nüìä Top 20 frequent cleaned words:\")\n",
    "print(cleaned_word_counts_df.head(20))\n",
    "\n",
    "# 2Ô∏è‚É£ Detect units\n",
    "unit_counts = detect_units(df)\n",
    "print(\"\\nüì¶ Unit counts found:\")\n",
    "print(unit_counts)\n",
    "\n",
    "# 3Ô∏è‚É£ Apply normalized & cleaned text in-place\n",
    "df = normalize_and_clean_simple(df)\n",
    "print(\"\\nüìù Sample cleaned_text & clean_words:\")\n",
    "print(df[['product_name', 'cleaned_text', 'clean_words']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f955434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_units(text):\n",
    "    # Standardize units\n",
    "    text = re.sub(r\"\\bML\\b\", \"ŸÖŸÑ\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bml\\b\", \"ŸÖŸÑ\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bKG\\b\", \"ŸÉŸäŸÑŸà\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bKg\\b\", \"ŸÉŸäŸÑŸà\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bkg\\b\", \"ŸÉŸäŸÑŸà\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def normalize_and_clean_units(df, column='product_name'):\n",
    "    cleaned_texts = []\n",
    "    for idx, text in df[column].astype(str).items():\n",
    "        # Step 1: normalize units\n",
    "        text = normalize_units(text)\n",
    "        # Step 2: remove unwanted characters but keep Arabic + letters + digits\n",
    "        text_clean = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \" \", text)\n",
    "        text_clean = re.sub(r\"\\s+\", \" \", text_clean).strip()\n",
    "        # Step 3: remove stopwords\n",
    "        words = [w for w in text_clean.split() if w not in arabic_stopwords]\n",
    "        cleaned_texts.append(\" \".join(words))\n",
    "    df['cleaned_text'] = cleaned_texts\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "228846c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "Arabic     464282\n",
      "English     33256\n",
      "Mixed        2462\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def detect_language(text):\n",
    "    text = str(text)\n",
    "    # Count Arabic letters\n",
    "    arabic_count = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "    # Count English letters\n",
    "    english_count = len(re.findall(r'[a-zA-Z]', text))\n",
    "    \n",
    "    if arabic_count > english_count:\n",
    "        return 'Arabic'\n",
    "    elif english_count > arabic_count:\n",
    "        return 'English'\n",
    "    else:\n",
    "        return 'Mixed'\n",
    "\n",
    "# Apply to product names\n",
    "df['language'] = df['product_name'].apply(detect_language)\n",
    "\n",
    "# Count how many products in each language\n",
    "language_counts = df['language'].value_counts()\n",
    "print(language_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c17d96ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚öôÔ∏è CREATING WEIGHTED SCORES\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: CREATE WEIGHTED SCORES & ID MAPPINGS\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"‚öôÔ∏è CREATING WEIGHTED SCORES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Event weights\n",
    "EVENT_WEIGHTS = {\n",
    "    'purchased': 5.0,\n",
    "    'cart': 3.0,\n",
    "    'rating': 2.5,\n",
    "    'wishlist': 2.0,\n",
    "    'search_keyword': 1.0\n",
    "}\n",
    "\n",
    "df['event_weight'] = df['event'].map(EVENT_WEIGHTS).fillna(1.0)\n",
    "\n",
    "# Recency weight\n",
    "reference_date = df['event_date'].max()\n",
    "df['days_ago'] = (reference_date - df['event_date']).dt.days\n",
    "df['recency_weight'] = np.exp(-0.01 * df['days_ago'])\n",
    "\n",
    "# Combined score\n",
    "df['score'] = df['event_weight'] * df['recency_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7743068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm users (‚â• 2 interactions): 49,359\n",
      "Cold users (< 2 interactions): 384,428\n"
     ]
    }
   ],
   "source": [
    "# Define warm user threshold\n",
    "warm_user_threshold = 2\n",
    "\n",
    "# Count interactions per user\n",
    "user_counts = df.groupby('customer_id').size()\n",
    "\n",
    "# Identify warm and cold users\n",
    "warm_users = set(user_counts[user_counts >= warm_user_threshold].index)\n",
    "cold_users = set(user_counts[user_counts < warm_user_threshold].index)\n",
    "\n",
    "print(f\"Warm users (‚â• {warm_user_threshold} interactions): {len(warm_users):,}\")\n",
    "print(f\"Cold users (< {warm_user_threshold} interactions): {len(cold_users):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e8140",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dff0a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 433,787\n",
      "Total products: 200,325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode user and product IDs\n",
    "user_encoder = LabelEncoder()\n",
    "product_encoder = LabelEncoder()\n",
    "\n",
    "df['user_idx'] = user_encoder.fit_transform(df['customer_id'])\n",
    "df['product_idx'] = product_encoder.fit_transform(df['product_id'])\n",
    "\n",
    "# Create mapping dictionaries\n",
    "mappings = {\n",
    "    'user_id_to_idx': dict(zip(df['customer_id'], df['user_idx'])),\n",
    "    'idx_to_user_id': dict(zip(df['user_idx'], df['customer_id'])),\n",
    "    'product_id_to_idx': dict(zip(df['product_id'], df['product_idx'])),\n",
    "    'idx_to_product_id': dict(zip(df['product_idx'], df['product_id'])),\n",
    "    'n_users': df['user_idx'].nunique(),\n",
    "    'n_products': df['product_idx'].nunique()\n",
    "}\n",
    "\n",
    "print(f\"Total users: {mappings['n_users']:,}\")\n",
    "print(f\"Total products: {mappings['n_products']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0307eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8247920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
